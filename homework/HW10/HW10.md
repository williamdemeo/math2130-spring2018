## HW 10

**Due: Friday, April 20, 9am.**

### Exercises
**Section 5.3:** 8, 16, 22  
**Section 5.4:** 2, 12, (21), (25), 26, (27)  
**Section 6.1:** 6, 10, 14, 20  

---

### Section 5.3
#### Exercises: 8, 16, 22  
Diagonalize the matrices in Exercises 8 and 16, if possible. 
The eigenvalues for Exercise 16 are $1$ and $2$.
<!-- (11) $\lambda = 1, 2, 3$;
(12) $\lambda = 2, 8$; 
(13) $\lambda =5, 1$; 
(14) $\lambda =5, 4$; 
(15) $\lambda =3, 1$;  -->

**5.3.8.**
$\left[\begin{array}{r}
5 & 1 \\ 0 & 5\end{array}\right]$

**5.3.16.**
$\left[\begin{array}{r}
0 & -4 & -6 \\ -1 & 0 & -3\\1 & 2 & 5\end{array}\right]$

**5.3.22.** Let $A$, $B$, $P$, and $D$ be $n \times n$ matrices.
Mark each statement True or False. Justify each answer. (Study
Theorems 5 and 6 and the examples in this section carefully before
you try this exercise.)

**a.** $A$ is diagonalizable if $A$ has $n$-eigenvectors.

**b.** If $A$ is diagonalizable, then $A$ has $n$ distinct eigenvalues.

**c.** If $AP=PD$, with $D$ diagonal, then the nonzero columns
of $P$ must be eigenvectors of $A$.

**d.** If $A$ is invertible, then $A$ is diagonalizable.

---

### Section 5.4
#### Exercises: 2, 12, (21), (25), 26, (27)  

**5.4.2.**
Let $\mathcal{D} = \{\mathbf{d}_1, \mathbf{d}_2\}$ 
and $\mathcal{B} = \{\mathbf{b}_1, \mathbf{b}_2\}$ 
be bases for vector spaces $V$ and $W$, respectively. 
Let $T :  V \to W$ be a linear transformation satisfying
\[
T (\mathbf{d}_1) = 2\mathbf{b}_1 - 3\mathbf{b}_2 \quad \text{ and } \quad
T (\mathbf{d}_2) = -4\mathbf{b}_1 + 5\mathbf{b}_2.
\]
Find the matrix for $T$ relative to $\mathcal{D}$ and $\mathcal{B}$.

**5.4.12.**
Find the $\mathcal{B}$-matrix for the transformation
$\mathbf{x} \mapsto A \mathbf{x}$, when 
$\mathcal{B} = \{\mathbf{b}_1, \mathbf{b}_2\}$, where 
\[A = \left[\begin{array}{r}
-1 & 4\\-2& 3 \end{array}\right],
\quad \mathbf{b}_1 = \left[\begin{array}{r}
3\\2 \end{array}\right], \quad
\mathbf{b}_2 =  â€ƒ\left[\begin{array}{r}
-1 \\ 1\end{array}\right].
\]

**5.4.21.** (recommended)   
Prove the following statement for square matrices, $A$, $B$, $C$.  If $B$ is similar to $A$ and $C$ is similar to $A$, then $B$ is similar
to $C$.

**5.4.25.** (recommended)  
The *trace* of a square matrix $A$ is the sum of the diagonal
entries in $A$ and is denoted by $\operatorname{tr} A$. 
It can be verified that $\operatorname{tr} (F G) = \operatorname{tr}(G F)$
for any two $n\times n$ matrices $F$ and $G$.
Show that if $A$ and $B$ are similar, then 
$\operatorname{tr} A = \operatorname{tr} B$.

**5.4.26.** It can be shown that the *trace* (see Exercise 5.4.25 for definition) 
of a matrix $A$ equals the sum of the eigenvalues of $A$. Verify this statement for the case when $A$ is diagonalizable.

**5.4.27.** (recommended)  
Let $V$ be $\mathbb{R}^n$ with a basis $\mathcal{B} = \{\mathbf{b}_1, \dots, \mathbf{b}_n\}$, let 
$W$ be $\mathbb{R}^n$ with a the standard basis, denoted $\mathcal{E}$.
Consider the identity transformation $I : V \to W$, where $I(\mathbf{x}) = \mathbf{x}$.
Find the matrix for $I$ relative to $\mathcal{B}$ and $\mathcal{E}$. 
What was this matrix called in Section 4.4?

---

### Section 6.1
#### Exercises: 6, 10, 14, 20  

**6.1.6.** Let
$\mathbf{w} = \left[\begin{array}{r}
3\\-1\\-5\end{array}\right]$
and $\mathbf{x} = \left[\begin{array}{r}
6\\-2\\3\end{array}\right]$. Compute
${\large \left(\frac{\mathbf{x} \cdot \mathbf{w}}{\mathbf{x}\cdot \mathbf{x}}\right)}$ $\mathbf{x}$

**6.1.10.**
Find a unit vector in the direction of the vector 
$\left[\begin{array}{r}
-6\\4\\-3\end{array}\right]$

**6.1.14.**
Find the distance between
$\mathbf{u} = \left[\begin{array}{r}
0\\-5\\2\end{array}\right]$
and 
$\mathbf{z} = \left[\begin{array}{r}
-4\\-1\\8\end{array}\right]$


**6.1.20.** Mark each statement True or False. Justify each answer.
All vectors are assumed to be in $\mathbb{R}^n$. 

**a.** $\mathbf{u} \cdot \mathbf{v} - \mathbf{v}\cdot \mathbf{u} = 0$

**b.** For any scalar $c$, $\|c\mathbf{v}\|= c\|\mathbf{v}\|$.

**c.** If $\mathbf{x}$ is orthogonal to every vector in a subspace $W$, 
then $\mathbf{x}$ is in $W^{\bot}$.

**d.** If 
$\|\mathbf{u}\|^2 + \|\mathbf{v}\|^2  = \|\mathbf{u} + \mathbf{v}\|^2$, 
then $\mathbf{u}$ and $\mathbf{v}$ are orthogonal.

**e.** For an $m \times n$ matrix $A$, vectors in the null space of $A$ are
orthogonal to vectors in the row space of $A$.
